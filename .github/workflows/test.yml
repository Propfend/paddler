name: Test Llama.cpp Integration

on:
  push:
    branches:
      - main
      - supervisor
  pull_request:
    branches:
      - main
      - supervisor

jobs:       
  test_dependencies:
    strategy:
      matrix:
        os: [ubuntu-latest]
    runs-on: ${{ matrix.os }}
    
    steps:
      - name: Download node
        run: |
          if [[ "${{ matrix.os }}" == "windows-latest" ]]; then
            winget install Schniz.fnm
            fnm install 22
          else
            curl -fsSL https://fnm.vercel.app/install | bash
            export PATH="$HOME/.local/share/fnm:$PATH"
            eval "$(fnm env)"
            fnm install 22
            fnm use 22
          fi
        shell: bash

      - name: Donwload prometheus
        run: |
          if [[ "${{ matrix.os }}" == "windows-latest" ]]; then
            curl.exe -L https://github.com/prometheus/prometheus/releases/download/v3.3.0-rc.1/prometheus-3.3.0-rc.1.windows-amd64.zip -o prometheus.zip
            Expand-Archive -Path .\prometheus.zip -DestinationPath .\prometheus
          else
            curl -L https://github.com/prometheus/prometheus/releases/download/v3.3.0-rc.1/prometheus-3.3.0-rc.1.linux-amd64.tar.gz -o prometheus.tar.gz
            tar -xvzf prometheus.tar.gz
          fi
        shell: bash
        
      - name: Download statsd
        run: |
          if [[ "${{ matrix.os }}" == "windows-latest" ]]; then
            curl.exe -L https://github.com/prometheus/statsd_exporter/releases/download/v0.27.2/statsd_exporter-0.27.2.windows-amd64.tar.gz -o statsd.tar.gz 
          else
            curl -L https://github.com/prometheus/statsd_exporter/releases/download/v0.27.2/statsd_exporter-0.27.2.linux-amd64.tar.gz -o statsd.tar.gz
          fi
          tar -xvzf statsd.tar.gz
        shell: bash

      - name: Download model
        run: |
          if [[ "${{ matrix.os }}" == "windows-latest" ]]; then
            powershell -Command Invoke-WebRequest -Uri 'https://huggingface.co/lmstudio-community/Qwen2-500M-Instruct-GGUF/resolve/main/Qwen2-500M-Instruct-IQ4_XS.gguf' -OutFile qwen2_500m.gguf
          elif [[ "${{ matrix.os }}" == "linux" ]]; then
            wget -O qwen2_500m.gguf https://huggingface.co/lmstudio-community/Qwen2-500M-Instruct-GGUF/resolve/main/Qwen2-500M-Instruct-IQ4_XS.gguf
          else
            curl -L -o qwen2_500m.gguf https://huggingface.co/lmstudio-community/Qwen2-500M-Instruct-GGUF/resolve/main/Qwen2-500M-Instruct-IQ4_XS.gguf
          fi
        shell: bash

      - name: Download llama.cpp
        run: |
          git clone https://github.com/ggml-org/llama.cpp.git
          cd llama.cpp
          git reset --hard f52d59d771dc231fc2ac39adacf157ddefc97730
          git clean -df f52d59d771dc231fc2ac39adacf157ddefc97730

          if [[ "${{ matrix.os }}" == "windows-latest" ]]; then
            cmake .
            cmake --build .
          else
            cmake -B build
            cmake --build build --config Release
          fi
        shell: bash

  test:
    needs: test_dependencies
    strategy:
      matrix:
        os: [ubuntu-latest]
    runs-on: ${{ matrix.os }}
    
    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@nightly
        with:
          components: rustfmt, clippy

      - name: Cache dependencies
        uses: Swatinem/rust-cache@v2

      # - name: Check formatting
      #   run: cargo fmt --check

      # - name: Clippy
      #   run: cargo clippy -- -D warnings

      - name: Run tests
        run: MODEL_NAME=qwen2_500m.gguf BINARY_NAME=llama.cpp/build/bin/llama-server STATSD_NAME=statsd_exporter-0.27.2.linux-amd64/statsd_exporter PROMETHEUS_NAME=prometheus-3.3.0-rc.1.linux-amd64/prometheus make test