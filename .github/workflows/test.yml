name: Test Llama.cpp Integration

on:
  push:
    branches:
      - main
      - supervisor
  pull_request:
    branches:
      - main
      - supervisor

jobs:       
  test_dependencies:
    strategy:
      matrix:
        os: [ubuntu-latest]
    runs-on: ${{ matrix.os }}
    
    steps:
      - name: Download node
        run: |
          if [[ "${{ matrix.os }}" == "windows-latest" ]]; then
            winget install Schniz.fnm
            fnm install 22
          else
            curl -fsSL https://fnm.vercel.app/install | bash
            export PATH="$HOME/.local/share/fnm:$PATH"
            eval "$(fnm env)"
            fnm install 22
            fnm use 22
          fi
        shell: bash

      - name: Donwload prometheus
        run: |
          if [[ "${{ matrix.os }}" == "windows-latest" ]]; then
            curl.exe -L https://github.com/prometheus/prometheus/releases/download/v3.3.0-rc.1/prometheus-3.3.0-rc.1.windows-amd64.zip -o prometheus.zip
            Expand-Archive -Path .\prometheus.zip -DestinationPath .\prometheus
          else
            curl -L https://github.com/prometheus/prometheus/releases/download/v3.3.0-rc.1/prometheus-3.3.0-rc.1.linux-amd64.tar.gz -o prometheus.tar.gz
            tar -xvzf prometheus.tar.gz
          fi
        shell: bash
        
      - name: Download statsd
        run: |
          if [[ "${{ matrix.os }}" == "windows-latest" ]]; then
            curl.exe -L https://github.com/prometheus/statsd_exporter/releases/download/v0.27.2/statsd_exporter-0.27.2.windows-amd64.tar.gz -o statsd.tar.gz 
          else
            curl -L https://github.com/prometheus/statsd_exporter/releases/download/v0.27.2/statsd_exporter-0.27.2.linux-amd64.tar.gz -o statsd.tar.gz
          fi
          tar -xvzf statsd.tar.gz
        shell: bash

  test:
    needs: test_dependencies
    strategy:
      matrix:
        os: [ubuntu-latest]
    runs-on: ${{ matrix.os }}
    
    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@nightly
        with:
          components: rustfmt, clippy

      - name: Cache dependencies
        uses: Swatinem/rust-cache@v2

      # - name: Check formatting
      #   run: cargo fmt --check

      # - name: Clippy
      #   run: cargo clippy -- -D warnings

      - name: Run tests
        run: make test


# name: Test Llama.cpp Integration

# on:
#   push:
#     branches:
#       - main
#       - supervisor
#   pull_request:
#     branches:
#       - main
#       - supervisor

# jobs:
#   test:
#     strategy:
#       matrix:
#         os: [ubuntu-latest]
#     runs-on: ${{ matrix.os }}
    
#     steps:
#       - uses: actions/checkout@v4
#         with:
#           submodules: recursive

#       - name: Install system dependencies
#         run: |
#           sudo apt-get update
#           sudo apt-get install -y build-essential cmake npm

#       - name: Install Rust
#         uses: dtolnay/rust-toolchain@nightly
#         with:
#           components: rustfmt, clippy

#       - name: Cache dependencies
#         uses: Swatinem/rust-cache@v2

#       - name: Install Node.js
#         uses: actions/setup-node@v3
#         with:
#           node-version: '20'

#       - name: Build llama.cpp
#         run: |
#           cd llama.cpp
#           mkdir -p build
#           cd build
#           cmake ..
#           cmake --build . --config Release
#           cd ../..

#       - name: Download test model
#         run: |
#           wget https://huggingface.co/lmstudio-community/Qwen2-500M-Instruct-GGUF/resolve/main/Qwen2-500M-Instruct-IQ4_XS.gguf -O qwen2_500m.gguf || true

#       - name: Run tests
#         env:
#           LLAMA_MODEL_PATH: qwen2_500m.gguf
#           RUST_LOG: debug
#         run: |
#           make test
