name: Test Llama.cpp Integration

on:
  push:
    branches:
      - main
      - cicd
  pull_request:
      branches:
      - main
      - cicd

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - name: Build Llama.cpp
        run: |
          git clone https://github.com/ggml-org/llama.cpp.git
          cd llama.cpp
          cmake -B build
          cmake --build build --config Release

      - name: Download model
        run: |
          wget -O qwen2.gguf https://huggingface.co/lmstudio-community/Qwen2-500M-Instruct-GGUF/resolve/main/Qwen2-500M-Instruct-IQ4_XS.gguf?download=true

      - name: Start Llama.cpp Server
        run: |
          llama.cpp/build/bin/llama-server -m qwen2.gguf --port 8088 --slots &
        shell: bash

      - name: checkout code
        uses: actions/checkout@v4

      - name: set up Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: stable

      - name: Cache dependencies
        uses: Swatinem/rust-cache@v2

      - name: build static
        run: |
          make esbuild

      - name: Build Rust Binary
        run: cargo build --features web_dashboard --release

      - name: Run loadbalancer
        run: target/release/paddler balancer --management-addr "localhost:8085" --reverseproxy-addr "localhost:8086" --management-dashboard-enable &

      - name: Run agent
        run: target/release/paddler agent --external-llamacpp-addr "localhost:8088" --local-llamacpp-addr "localhost:8088" --management-addr "localhost:8085" &
